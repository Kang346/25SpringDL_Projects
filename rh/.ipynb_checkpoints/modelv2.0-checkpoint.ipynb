{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eda62191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb44875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Skip connection with 1x1 convolution when downsampling\n",
    "        self.downsample = downsample\n",
    "        if downsample:\n",
    "            self.skip_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "            self.skip_bn = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            self.skip_conv = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample:\n",
    "            identity = self.skip_bn(self.skip_conv(x))\n",
    "\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity  \n",
    "        return F.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f62b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomResNet, self).__init__()\n",
    "\n",
    "        # Initial Convolution (C1 = 32 channels)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Residual layers with Max Pooling after each layer\n",
    "        self.layer1 = self._make_layer(32, 32, num_blocks=2, stride=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling after layer1\n",
    "\n",
    "        self.layer2 = self._make_layer(32, 64, num_blocks=2, stride=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling after layer2\n",
    "\n",
    "        self.layer3 = self._make_layer(64, 128, num_blocks=2, stride=2)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling after layer3\n",
    "\n",
    "        self.layer4 = self._make_layer(128, 256, num_blocks=2, stride=2)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling after layer4\n",
    "\n",
    "        # Average Pooling (Final layer)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 800),  # First FC layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(800, num_classes)  # Output layer\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride=stride, downsample=True))\n",
    "        layers.append(ResidualBlock(out_channels, out_channels, stride=1, downsample=False))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.pool1(x)  # Apply max pooling after layer1\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.pool2(x)  # Apply max pooling after layer2\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.pool3(x)  # Apply max pooling after layer3\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        x = self.pool4(x)  # Apply max pooling after layer4\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b04869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "001a0109",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomResNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "297dfd7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (256x1x1). Calculated output size: (256x0x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Print model summary (assuming input image size is 3×32×32 for CIFAR-10)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 52\u001b[0m, in \u001b[0;36mCustomResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool3(x)  \u001b[38;5;66;03m# Apply max pooling after layer3\u001b[39;00m\n\u001b[0;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[1;32m---> 52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Apply max pooling after layer4\u001b[39;00m\n\u001b[0;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1798\u001b[0m     ):\n\u001b[0;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given input size: (256x1x1). Calculated output size: (256x0x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Move model to the correct device (CPU or GPU)\n",
    "model.to(device)\n",
    "\n",
    "# Print model summary (assuming input image size is 3×32×32 for CIFAR-10)\n",
    "summary(model, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e980e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbb5f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_batches():\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for i in range(1, 6):  # data_batch_1 to data_batch_5\n",
    "        batch_file = os.path.join(data_dir, f\"data_batch_{i}\")\n",
    "        batch_dict = unpickle(batch_file)\n",
    "        \n",
    "        batch_data = batch_dict[b'data']  # Image data (10000, 3072)\n",
    "        batch_labels = batch_dict[b'labels']  # Labels (10000,)\n",
    "        \n",
    "        train_data.append(batch_data)\n",
    "        train_labels.extend(batch_labels)\n",
    "    \n",
    "    train_data = np.vstack(train_data).reshape(-1, 3, 32, 32).astype(np.float32) / 255.0  # Normalize\n",
    "    train_labels = np.array(train_labels)\n",
    "    \n",
    "    return train_data, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65ef54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_test():\n",
    "    test_file = os.path.join(data_dir, \"test_batch\")\n",
    "    test_dict = unpickle(test_file)\n",
    "    \n",
    "    test_data = test_dict[b'data'].reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "    test_labels = np.array(test_dict[b'labels'])\n",
    "    \n",
    "    return test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85386ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data # (N, 3, 32, 32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = np.transpose(self.data[idx], (1, 2, 0))  # (3, 32, 32) -> (32, 32, 3)\n",
    "\n",
    "        # to PIL\n",
    "        img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3548e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)  # Get predicted class\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_acc = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26c97c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/cifar-10-batches-py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af36126d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CIFAR-10: 40000 train, 10000 val, 10000 test samples.\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_data, train_labels = load_cifar10_batches()\n",
    "test_data, test_labels = load_cifar10_test()\n",
    "\n",
    "# Split Training Set into Training & Validation (80% Train, 20% Validation)\n",
    "train_size = int(0.8 * len(train_data))\n",
    "val_size = len(train_data) - train_size\n",
    "train_data, val_data = train_data[:train_size], train_data[train_size:]\n",
    "train_labels, val_labels = train_labels[:train_size], train_labels[train_size:]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Create Dataset & DataLoader\n",
    "train_dataset = CIFAR10Dataset(train_data, train_labels, transform=transform)\n",
    "val_dataset = CIFAR10Dataset(val_data, val_labels, transform=transform)\n",
    "test_dataset = CIFAR10Dataset(test_data, test_labels, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Loaded CIFAR-10: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0e1dd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Epoch 1/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 2.0646\n",
      "🟢 [Batch 100/313] Loss: 1.9740\n",
      "🟢 [Batch 150/313] Loss: 1.7989\n",
      "🟢 [Batch 200/313] Loss: 1.7147\n",
      "🟢 [Batch 250/313] Loss: 1.8186\n",
      "🟢 [Batch 300/313] Loss: 1.5945\n",
      "🟢 [Batch 313/313] Loss: 1.8621\n",
      "✅ Training Loss: 1.8486, Training Accuracy: 30.82%\n",
      " Learning Rate after Epoch 1: 0.099384\n",
      "🔵 Validation Accuracy: 40.50%\n",
      "💾 Best model saved with Validation Accuracy: 40.50%\n",
      "\n",
      "🔄 Epoch 2/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 1.5986\n",
      "🟢 [Batch 100/313] Loss: 1.6714\n",
      "🟢 [Batch 150/313] Loss: 1.3135\n",
      "🟢 [Batch 200/313] Loss: 1.5322\n",
      "🟢 [Batch 250/313] Loss: 1.2145\n",
      "🟢 [Batch 300/313] Loss: 1.4726\n",
      "🟢 [Batch 313/313] Loss: 1.3786\n",
      "✅ Training Loss: 1.4338, Training Accuracy: 48.10%\n",
      " Learning Rate after Epoch 2: 0.097553\n",
      "🔵 Validation Accuracy: 52.38%\n",
      "💾 Best model saved with Validation Accuracy: 52.38%\n",
      "\n",
      "🔄 Epoch 3/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 1.1213\n",
      "🟢 [Batch 100/313] Loss: 1.0126\n",
      "🟢 [Batch 150/313] Loss: 1.1907\n",
      "🟢 [Batch 200/313] Loss: 1.0657\n",
      "🟢 [Batch 250/313] Loss: 1.3086\n",
      "🟢 [Batch 300/313] Loss: 1.2027\n",
      "🟢 [Batch 313/313] Loss: 1.0457\n",
      "✅ Training Loss: 1.1632, Training Accuracy: 58.13%\n",
      " Learning Rate after Epoch 3: 0.094550\n",
      "🔵 Validation Accuracy: 58.69%\n",
      "💾 Best model saved with Validation Accuracy: 58.69%\n",
      "\n",
      "🔄 Epoch 4/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 1.0298\n",
      "🟢 [Batch 100/313] Loss: 0.8978\n",
      "🟢 [Batch 150/313] Loss: 0.9875\n",
      "🟢 [Batch 200/313] Loss: 1.1731\n",
      "🟢 [Batch 250/313] Loss: 0.8692\n",
      "🟢 [Batch 300/313] Loss: 0.8936\n",
      "🟢 [Batch 313/313] Loss: 0.7935\n",
      "✅ Training Loss: 0.9859, Training Accuracy: 65.33%\n",
      " Learning Rate after Epoch 4: 0.090451\n",
      "🔵 Validation Accuracy: 67.08%\n",
      "💾 Best model saved with Validation Accuracy: 67.08%\n",
      "\n",
      "🔄 Epoch 5/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.9168\n",
      "🟢 [Batch 100/313] Loss: 0.7465\n",
      "🟢 [Batch 150/313] Loss: 0.7617\n",
      "🟢 [Batch 200/313] Loss: 0.9437\n",
      "🟢 [Batch 250/313] Loss: 0.9562\n",
      "🟢 [Batch 300/313] Loss: 0.6633\n",
      "🟢 [Batch 313/313] Loss: 0.9522\n",
      "✅ Training Loss: 0.8586, Training Accuracy: 70.06%\n",
      " Learning Rate after Epoch 5: 0.085355\n",
      "🔵 Validation Accuracy: 68.66%\n",
      "💾 Best model saved with Validation Accuracy: 68.66%\n",
      "\n",
      "🔄 Epoch 6/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.6826\n",
      "🟢 [Batch 100/313] Loss: 0.8249\n",
      "🟢 [Batch 150/313] Loss: 0.7636\n",
      "🟢 [Batch 200/313] Loss: 0.7585\n",
      "🟢 [Batch 250/313] Loss: 0.5095\n",
      "🟢 [Batch 300/313] Loss: 0.6762\n",
      "🟢 [Batch 313/313] Loss: 0.9425\n",
      "✅ Training Loss: 0.7698, Training Accuracy: 73.64%\n",
      " Learning Rate after Epoch 6: 0.079389\n",
      "🔵 Validation Accuracy: 74.07%\n",
      "💾 Best model saved with Validation Accuracy: 74.07%\n",
      "\n",
      "🔄 Epoch 7/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.7195\n",
      "🟢 [Batch 100/313] Loss: 0.8571\n",
      "🟢 [Batch 150/313] Loss: 0.6931\n",
      "🟢 [Batch 200/313] Loss: 0.6891\n",
      "🟢 [Batch 250/313] Loss: 0.6401\n",
      "🟢 [Batch 300/313] Loss: 0.6426\n",
      "🟢 [Batch 313/313] Loss: 0.7789\n",
      "✅ Training Loss: 0.6905, Training Accuracy: 76.28%\n",
      " Learning Rate after Epoch 7: 0.072700\n",
      "🔵 Validation Accuracy: 74.03%\n",
      "\n",
      "🔄 Epoch 8/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.6099\n",
      "🟢 [Batch 100/313] Loss: 0.5419\n",
      "🟢 [Batch 150/313] Loss: 0.6848\n",
      "🟢 [Batch 200/313] Loss: 0.4889\n",
      "🟢 [Batch 250/313] Loss: 0.7250\n",
      "🟢 [Batch 300/313] Loss: 0.7330\n",
      "🟢 [Batch 313/313] Loss: 0.6351\n",
      "✅ Training Loss: 0.6406, Training Accuracy: 78.06%\n",
      " Learning Rate after Epoch 8: 0.065451\n",
      "🔵 Validation Accuracy: 77.24%\n",
      "💾 Best model saved with Validation Accuracy: 77.24%\n",
      "\n",
      "🔄 Epoch 9/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.5727\n",
      "🟢 [Batch 100/313] Loss: 0.5733\n",
      "🟢 [Batch 150/313] Loss: 0.5774\n",
      "🟢 [Batch 200/313] Loss: 0.5167\n",
      "🟢 [Batch 250/313] Loss: 0.6606\n",
      "🟢 [Batch 300/313] Loss: 0.6213\n",
      "🟢 [Batch 313/313] Loss: 0.4339\n",
      "✅ Training Loss: 0.5862, Training Accuracy: 79.89%\n",
      " Learning Rate after Epoch 9: 0.057822\n",
      "🔵 Validation Accuracy: 79.64%\n",
      "💾 Best model saved with Validation Accuracy: 79.64%\n",
      "\n",
      "🔄 Epoch 10/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.4371\n",
      "🟢 [Batch 100/313] Loss: 0.5770\n",
      "🟢 [Batch 150/313] Loss: 0.6146\n",
      "🟢 [Batch 200/313] Loss: 0.5367\n",
      "🟢 [Batch 250/313] Loss: 0.5081\n",
      "🟢 [Batch 300/313] Loss: 0.6508\n",
      "🟢 [Batch 313/313] Loss: 0.7555\n",
      "✅ Training Loss: 0.5381, Training Accuracy: 81.54%\n",
      " Learning Rate after Epoch 10: 0.050000\n",
      "🔵 Validation Accuracy: 80.69%\n",
      "💾 Best model saved with Validation Accuracy: 80.69%\n",
      "\n",
      "🔄 Epoch 11/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.4986\n",
      "🟢 [Batch 100/313] Loss: 0.5622\n",
      "🟢 [Batch 150/313] Loss: 0.6000\n",
      "🟢 [Batch 200/313] Loss: 0.4066\n",
      "🟢 [Batch 250/313] Loss: 0.5469\n",
      "🟢 [Batch 300/313] Loss: 0.5010\n",
      "🟢 [Batch 313/313] Loss: 0.5199\n",
      "✅ Training Loss: 0.5004, Training Accuracy: 82.67%\n",
      " Learning Rate after Epoch 11: 0.042178\n",
      "🔵 Validation Accuracy: 79.92%\n",
      "\n",
      "🔄 Epoch 12/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.4360\n",
      "🟢 [Batch 100/313] Loss: 0.5114\n",
      "🟢 [Batch 150/313] Loss: 0.6481\n",
      "🟢 [Batch 200/313] Loss: 0.3843\n",
      "🟢 [Batch 250/313] Loss: 0.3825\n",
      "🟢 [Batch 300/313] Loss: 0.3805\n",
      "🟢 [Batch 313/313] Loss: 0.3833\n",
      "✅ Training Loss: 0.4534, Training Accuracy: 84.36%\n",
      " Learning Rate after Epoch 12: 0.034549\n",
      "🔵 Validation Accuracy: 82.81%\n",
      "💾 Best model saved with Validation Accuracy: 82.81%\n",
      "\n",
      "🔄 Epoch 13/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.4741\n",
      "🟢 [Batch 100/313] Loss: 0.3994\n",
      "🟢 [Batch 150/313] Loss: 0.3615\n",
      "🟢 [Batch 200/313] Loss: 0.4617\n",
      "🟢 [Batch 250/313] Loss: 0.4536\n",
      "🟢 [Batch 300/313] Loss: 0.3260\n",
      "🟢 [Batch 313/313] Loss: 0.4490\n",
      "✅ Training Loss: 0.4143, Training Accuracy: 85.80%\n",
      " Learning Rate after Epoch 13: 0.027300\n",
      "🔵 Validation Accuracy: 82.56%\n",
      "\n",
      "🔄 Epoch 14/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.4201\n",
      "🟢 [Batch 100/313] Loss: 0.2572\n",
      "🟢 [Batch 150/313] Loss: 0.3754\n",
      "🟢 [Batch 200/313] Loss: 0.2711\n",
      "🟢 [Batch 250/313] Loss: 0.2115\n",
      "🟢 [Batch 300/313] Loss: 0.3618\n",
      "🟢 [Batch 313/313] Loss: 0.3979\n",
      "✅ Training Loss: 0.3772, Training Accuracy: 87.03%\n",
      " Learning Rate after Epoch 14: 0.020611\n",
      "🔵 Validation Accuracy: 83.64%\n",
      "💾 Best model saved with Validation Accuracy: 83.64%\n",
      "\n",
      "🔄 Epoch 15/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.4867\n",
      "🟢 [Batch 100/313] Loss: 0.5075\n",
      "🟢 [Batch 150/313] Loss: 0.3799\n",
      "🟢 [Batch 200/313] Loss: 0.3123\n",
      "🟢 [Batch 250/313] Loss: 0.3068\n",
      "🟢 [Batch 300/313] Loss: 0.2119\n",
      "🟢 [Batch 313/313] Loss: 0.2727\n",
      "✅ Training Loss: 0.3277, Training Accuracy: 88.80%\n",
      " Learning Rate after Epoch 15: 0.014645\n",
      "🔵 Validation Accuracy: 86.32%\n",
      "💾 Best model saved with Validation Accuracy: 86.32%\n",
      "\n",
      "🔄 Epoch 16/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.2825\n",
      "🟢 [Batch 100/313] Loss: 0.2351\n",
      "🟢 [Batch 150/313] Loss: 0.2521\n",
      "🟢 [Batch 200/313] Loss: 0.2782\n",
      "🟢 [Batch 250/313] Loss: 0.3802\n",
      "🟢 [Batch 300/313] Loss: 0.2286\n",
      "🟢 [Batch 313/313] Loss: 0.2497\n",
      "✅ Training Loss: 0.2929, Training Accuracy: 90.00%\n",
      " Learning Rate after Epoch 16: 0.009549\n",
      "🔵 Validation Accuracy: 87.17%\n",
      "💾 Best model saved with Validation Accuracy: 87.17%\n",
      "\n",
      "🔄 Epoch 17/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.3582\n",
      "🟢 [Batch 100/313] Loss: 0.2653\n",
      "🟢 [Batch 150/313] Loss: 0.3158\n",
      "🟢 [Batch 200/313] Loss: 0.2362\n",
      "🟢 [Batch 250/313] Loss: 0.2265\n",
      "🟢 [Batch 300/313] Loss: 0.2082\n",
      "🟢 [Batch 313/313] Loss: 0.1596\n",
      "✅ Training Loss: 0.2553, Training Accuracy: 91.09%\n",
      " Learning Rate after Epoch 17: 0.005450\n",
      "🔵 Validation Accuracy: 88.14%\n",
      "💾 Best model saved with Validation Accuracy: 88.14%\n",
      "\n",
      "🔄 Epoch 18/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.1958\n",
      "🟢 [Batch 100/313] Loss: 0.2142\n",
      "🟢 [Batch 150/313] Loss: 0.2129\n",
      "🟢 [Batch 200/313] Loss: 0.1942\n",
      "🟢 [Batch 250/313] Loss: 0.3138\n",
      "🟢 [Batch 300/313] Loss: 0.2498\n",
      "🟢 [Batch 313/313] Loss: 0.3077\n",
      "✅ Training Loss: 0.2234, Training Accuracy: 92.33%\n",
      " Learning Rate after Epoch 18: 0.002447\n",
      "🔵 Validation Accuracy: 88.83%\n",
      "💾 Best model saved with Validation Accuracy: 88.83%\n",
      "\n",
      "🔄 Epoch 19/20 ---------------------------\n",
      "🟢 [Batch 50/313] Loss: 0.1677\n",
      "🟢 [Batch 100/313] Loss: 0.1257\n",
      "🟢 [Batch 150/313] Loss: 0.1601\n",
      "🟢 [Batch 200/313] Loss: 0.2473\n",
      "🟢 [Batch 250/313] Loss: 0.1392\n",
      "🟢 [Batch 300/313] Loss: 0.2149\n",
      "🟢 [Batch 313/313] Loss: 0.2260\n",
      "✅ Training Loss: 0.1985, Training Accuracy: 93.06%\n",
      " Learning Rate after Epoch 19: 0.000616\n",
      "🔵 Validation Accuracy: 89.12%\n",
      "💾 Best model saved with Validation Accuracy: 89.12%\n",
      "\n",
      "🔄 Epoch 20/20 ---------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 [Batch 50/313] Loss: 0.2213\n",
      "🟢 [Batch 100/313] Loss: 0.1179\n",
      "🟢 [Batch 150/313] Loss: 0.1755\n",
      "🟢 [Batch 200/313] Loss: 0.0894\n",
      "🟢 [Batch 250/313] Loss: 0.1734\n",
      "🟢 [Batch 300/313] Loss: 0.2438\n",
      "🟢 [Batch 313/313] Loss: 0.2094\n",
      "✅ Training Loss: 0.1848, Training Accuracy: 93.70%\n",
      " Learning Rate after Epoch 20: 0.000000\n",
      "🔵 Validation Accuracy: 89.18%\n",
      "💾 Best model saved with Validation Accuracy: 89.18%\n",
      "\n",
      "🎉 Training Completed!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "best_val_acc = 0.0\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n🔄 Epoch {epoch+1}/{num_epochs} ---------------------------\")\n",
    "    \n",
    "    ### TRAINING PHASE ###\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Track accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Print loss for every 50 batches\n",
    "        if (batch_idx + 1) % 50 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            print(f\"[Batch {batch_idx+1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
    "    # Calculate training accuracy\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%\")\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\" Learning Rate after Epoch {epoch+1}: {current_lr:.6f}\")\n",
    "    \n",
    "    ### VALIDATION PHASE ###\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    ### SAVE BEST MODEL ###\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(f\"Best model saved with Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "print(\"\\n Training Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "756b8d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Test Accuracy: 88.57%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88.57"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8356e75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81364ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf540d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
